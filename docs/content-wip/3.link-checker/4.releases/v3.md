# v3.0.0

## Background

Working towards the v2 of Nuxt SEO Kit, we need to bump all dependencies a new major. This major is a move towards an easier way to manage site config through the [nuxt-site-config]() module.

With this major, we also introduce numerous DX improvements to make managing your robots, even simpler.

## Features :rocket:

### Robots.txt module configuration

The recommended way to configure this module is now to create a robots.txt file in either your `<rootDir>/assets` or `<rootDir>` directory.

The robots.txt will be parsed and will be merged in with the module config.

For more complex configurations, you can still use the module config, route rules, and hooks.

You can read more on the [documentation](https://github.com/harlan-zw/nuxt-simple-sitemap#robotstxt-configuration).

### Nuxt Site Config Integration

The module now integrates with the [nuxt-site-config](https://github.com/harlan-zw/nuxt-site-config) module.

The `siteUrl` and `indexable` config is now deprecated, but will still work.

For most sites, you won't need to provide any further configuration, everything will just work. If you need to modify
the default config, the easiest way is to do so through the `site` config.

```ts
export default defineNuxtConfig({
  site: {
    url: 'https://example.com',
    indexable: true
  }
})
```

### New Config: `groups`

- Type: `{ userAgent: []; allow: []; disallow: []; comments: [] }[]`
- Default: `[]`
- Required: `false`

Define more granular rules for the robots.txt. Each group is a set of rules for specific user agent(s).

```ts
export default defineNuxtConfig({
  robots: {
    groups: [
      {
        userAgents: ['AdsBot-Google-Mobile', 'AdsBot-Google-Mobile-Apps'],
        disallow: ['/admin'],
        allow: ['/admin/login'],
        comments: 'Allow Google AdsBot to index the login page but no-admin pages'
      },
    ]
  }
})
```


## Other Improvements

### Improved header / meta tag integration

Previously, only routes added to the `routeRules` would be used to display the `X-Robots-Tag` header and the `<meta name="robots" content="..." />` tag. 

This has been changed to include all `disallow` paths for the `*` user-agent by default.

### New Config: `blockNonSeoBots`

- Type: `boolean`
- Default: `false`
- Required: `false`

Blocks some non-SEO bots from crawling your site. This is not a replacement for a full-blown bot management solution, but it can help to reduce the load on your server.

See [const.ts](https://github.com/harlan-zw/nuxt-simple-sitemap/blob/main/src/const.ts#L6) for the list of bots that are blocked.

```ts
export default defineNuxtConfig({
  robots: {
    blockNonSeoBots: true
  }
})
```

### New Config: `credits`

- Type: `boolean`
- Default: `true`
- Required: `false`

Control the module credit comment in the generated robots.txt file.

```txt
# START nuxt-simple-sitemap (indexable) <- credits
 ...
# END nuxt-simple-sitemap <- credits
```

```ts
export default defineNuxtConfig({
  robots: {
    credits: false
  }
})
```

### New Config: `debug`

- Type: `boolean`
- Default: `false`
- Required: `false`

Enables debug logs.

```ts
export default defineNuxtConfig({
  robots: {
    debug: true
  }
})
```
